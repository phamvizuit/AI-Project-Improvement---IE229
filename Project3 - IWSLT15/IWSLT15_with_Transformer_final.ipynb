{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwXqa606hHH9"
      },
      "source": [
        "#1. Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L9EeZlzcT1iz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "import requests\n",
        "import tarfile\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PtaoTs7T65F"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPOCH = 30\n",
        "BATCHSIZE = 16\n",
        "LR = 0.0001\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7avVEkcGUCbK"
      },
      "outputs": [],
      "source": [
        "url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/'\n",
        "train_en = [line.split() for line in requests.get(url+'train.en').text.splitlines()]\n",
        "train_vi = [line.split() for line in requests.get(url+'train.vi').text.splitlines()]\n",
        "test_en = [line.split() for line in requests.get(url+'tst2013.en').text.splitlines()]\n",
        "test_vi = [line.split() for line in requests.get(url+'tst2013.vi').text.splitlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2SpVSaaUEBy",
        "outputId": "4c10ac26-ab72-4c86-c224-9b93e71368dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline']\n",
            "['Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n",
            "['In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.']\n",
            "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.']\n",
            "['I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.']\n",
            "['Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.']\n",
            "['Headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.']\n",
            "['Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.']\n",
            "['They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.']\n",
            "['Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.']\n",
            "['Recently', 'the', 'headlines', 'looked', 'like', 'this', 'when', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', ',', 'or', 'IPCC', ',', 'put', 'out', 'their', 'report', 'on', 'the', 'state', 'of', 'understanding', 'of', 'the', 'atmospheric', 'system', '.']\n",
            "['Các', 'tiêu', 'đề', 'gần', 'đây', 'trông', 'như', 'thế', 'này', 'khi', 'Ban', 'Điều', 'hành', 'Biến', 'đổi', 'khí', 'hậu', 'Liên', 'chính', 'phủ', ',', 'gọi', 'tắt', 'là', 'IPCC', 'đưa', 'ra', 'bài', 'nghiên', 'cứu', 'của', 'họ', 'về', 'hệ', 'thống', 'khí', 'quyển', '.']\n",
            "['That', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.']\n",
            "['Nghiên', 'cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa', 'học', 'từ', '40', 'quốc', 'gia', 'khác', 'nhau', '.']\n",
            "['They', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.']\n",
            "['Họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ', 'đề', 'này', '.']\n",
            "['And', 'all', 'of', 'those', 'pages', 'were', 'reviewed', 'by', 'another', '400-plus', 'scientists', 'and', 'reviewers', ',', 'from', '113', 'countries', '.']\n",
            "['Và', 'tất', 'cả', 'các', 'trang', 'đều', 'được', 'xem', 'xét', 'bởi', '400', 'khoa', 'học', 'gia', 'và', 'nhà', 'phê', 'bình', 'khác', 'từ', '113', 'quốc', 'gia', '.']\n",
            "['It', '&apos;s', 'a', 'big', 'community', '.', 'It', '&apos;s', 'such', 'a', 'big', 'community', ',', 'in', 'fact', ',', 'that', 'our', 'annual', 'gathering', 'is', 'the', 'largest', 'scientific', 'meeting', 'in', 'the', 'world', '.']\n",
            "['Đó', 'là', 'cả', 'một', 'cộng', 'đồng', 'lớn', ',', 'lớn', 'đến', 'nỗi', 'trên', 'thực', 'tế', 'cuộc', 'tụ', 'hội', 'hằng', 'năm', 'của', 'chúng', 'tôi', 'là', 'hội', 'nghị', 'khoa', 'học', '&#91;', 'tự', 'nhiên', '&#93;', 'lớn', 'nhất', 'thế', 'giới', '.']\n",
            "# of line 133317 133317 1268 1268\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(train_en[i])\n",
        "  print(train_vi[i])\n",
        "print('# of line', len(train_en), len(train_vi), len(test_en), len(test_vi))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make vocab"
      ],
      "metadata": {
        "id": "9CSCFKJDavQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvjwzf7VWjfw",
        "outputId": "6d73fc1c-04b4-4f2f-c433-a8516fa86c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size en:  24420\n",
            "vocab size vi:  10666\n"
          ]
        }
      ],
      "source": [
        "def make_vocab(train_data, min_freq):\n",
        "  vocab = {}\n",
        "  for tokenlist in train_data:\n",
        "    for token in tokenlist:\n",
        "      if token not in vocab:\n",
        "        vocab[token] = 0\n",
        "      vocab[token] += 1 # Đếm các từ data \n",
        "  vocablist = [('<unk>',0),('<pad>', 0), ('<cls>', 0), ('<eos>', 0)] # Danh sách các tuple đặc trưng trước\n",
        "  vocabidx = {} # dict của tuple\n",
        "  for token, freq in vocab.items(): # Từ đó , với số lượng của từ đó trong train data \n",
        "    if freq >= min_freq: # Số lượng từ mà > 3\n",
        "      idx = len(vocablist) # đang là 4 \n",
        "      vocablist.append((token, freq)) #Từ đó , với số lượng của từ đó trong train data \n",
        "      vocabidx[token] = idx  # Từ điển (dict)  \n",
        "  vocabidx['<unk>'] = 0\n",
        "  vocabidx['<pad>'] = 1\n",
        "  vocabidx['<cls>'] = 2\n",
        "  vocabidx['<eos>'] = 3\n",
        "  return vocablist, vocabidx\n",
        "\n",
        "vocablist_en, vocabidx_en = make_vocab(train_en, 3)\n",
        "vocablist_vi, vocabidx_vi = make_vocab(train_vi, 3)\n",
        "\n",
        "print('vocab size en: ', len(vocablist_en))\n",
        "print('vocab size vi: ', len(vocablist_vi))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "miWcxFcSaz_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xetCEvu-WqYm",
        "outputId": "b1c72f59-1fd1-4038-ae14-dcfd40492b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<cls>', 'Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
            "['<cls>', 'Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>']\n",
            "['<cls>', 'In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
            "['<cls>', 'Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.', '<eos>']\n",
            "['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>']\n",
            "['<cls>', 'I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
            "['<cls>', 'Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
            "['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
            "['<cls>', '<unk>', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
            "['<cls>', 'Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
            "['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
            "['<cls>', 'They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
            "['<cls>', 'Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "def preprocess(data, vocabidx):\n",
        "  rr = []\n",
        "  for tokenlist in data:\n",
        "    tkl =['<cls>']\n",
        "    for token in tokenlist:\n",
        "      tkl.append(token if token in vocabidx else '<unk>')\n",
        "    tkl.append('<eos>')\n",
        "    rr.append((tkl))\n",
        "  return rr\n",
        "  \n",
        "train_en_prep = preprocess(train_en, vocabidx_en)\n",
        "train_vi_prep = preprocess(train_vi, vocabidx_vi)\n",
        "test_en_prep = preprocess(test_en, vocabidx_en)\n",
        "for i in range(5):\n",
        "  print(train_en_prep[i])\n",
        "  print(train_vi_prep[i])\n",
        "  print(test_en_prep[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLu-AGK6WsfI",
        "outputId": "78046ee6-7502-4366-e8ee-68248a2de203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>'], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "(['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>'], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "(['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>'], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n",
            "(['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], ['Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.'], ['Mặc', 'dù', 'tôi', 'đã', 'từng', 'tự', 'hỏi', 'không', 'biết', 'thế', 'giới', 'bên', 'ngoài', 'kia', 'như', 'thế', 'nào', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'rằng', 'mình', 'sẽ', 'sống', 'cả', 'cuộc', 'đời', 'ở', 'BắcTriều', 'Tiên', ',', 'cho', 'tới', 'khi', 'tất', 'cả', 'mọi', 'thứ', 'đột', 'nhiên', 'thay', 'đổi', '.'])\n",
            "(['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>'], ['When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.'], ['Khi', 'tôi', 'lên', '7', ',', 'tôi', 'chứng', 'kiến', 'cảnh', 'người', 'ta', 'xử', 'bắn', 'công', 'khai', 'lần', 'đầu', 'tiên', 'trong', 'đời', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'cuộc', 'sống', 'của', 'mình', 'ở', 'đây', 'là', 'hoàn', 'toàn', 'bình', 'thường', '.'])\n",
            "(['<cls>', 'My', 'family', 'was', 'not', 'poor', ',', 'and', 'myself', ',', 'I', 'had', 'never', 'experienced', 'hunger', '.', '<eos>'], ['My', 'family', 'was', 'not', 'poor', ',', 'and', 'myself', ',', 'I', 'had', 'never', 'experienced', 'hunger', '.'], ['Gia', 'đình', 'của', 'tôi', 'không', 'nghèo', ',', 'và', 'bản', 'thân', 'tôi', 'thì', 'chưa', 'từng', 'phải', 'chịu', 'đói', '.'])\n",
            "(['<cls>', 'But', 'one', 'day', ',', 'in', '1995', ',', 'my', 'mom', 'brought', 'home', 'a', 'letter', 'from', 'a', 'coworker', '&apos;s', 'sister', '.', '<eos>'], ['But', 'one', 'day', ',', 'in', '1995', ',', 'my', 'mom', 'brought', 'home', 'a', 'letter', 'from', 'a', 'coworker', '&apos;s', 'sister', '.'], ['Nhưng', 'vào', 'một', 'ngày', 'của', 'năm', '1995', ',', 'mẹ', 'tôi', 'mang', 'về', 'nhà', 'một', 'lá', 'thư', 'từ', 'một', 'người', 'chị', 'em', 'cùng', 'chỗ', 'làm', 'với', 'mẹ', '.'])\n",
            "(['<cls>', 'It', 'read', ',', '&quot;', 'When', 'you', 'read', 'this', ',', 'all', 'five', 'family', 'members', 'will', 'not', 'exist', 'in', 'this', 'world', ',', 'because', 'we', 'haven', '&apos;t', 'eaten', 'for', 'the', 'past', 'two', 'weeks', '.', '<eos>'], ['It', 'read', ',', '&quot;', 'When', 'you', 'read', 'this', ',', 'all', 'five', 'family', 'members', 'will', 'not', 'exist', 'in', 'this', 'world', ',', 'because', 'we', 'haven', '&apos;t', 'eaten', 'for', 'the', 'past', 'two', 'weeks', '.'], ['Trong', 'đó', 'có', 'viết', ':', 'Khi', 'chị', 'đọc', 'được', 'những', 'dòng', 'này', 'thì', 'cả', 'gia', 'đình', '5', 'người', 'của', 'em', 'đã', 'không', 'còn', 'trên', 'cõi', 'đời', 'này', 'nữa', ',', 'bởi', 'vì', 'cả', 'nhà', 'em', 'đã', 'không', 'có', 'gì', 'để', 'ăn', 'trong', 'hai', 'tuần', '.'])\n",
            "(['<cls>', 'We', 'are', 'lying', 'on', 'the', 'floor', 'together', ',', 'and', 'our', 'bodies', 'are', 'so', 'weak', 'we', 'are', 'ready', 'to', 'die', '.', '&quot;', '<eos>'], ['We', 'are', 'lying', 'on', 'the', 'floor', 'together', ',', 'and', 'our', 'bodies', 'are', 'so', 'weak', 'we', 'are', 'ready', 'to', 'die', '.', '&quot;'], ['Tất', 'cả', 'cùng', 'nằm', 'trên', 'sàn', ',', 'và', 'cơ', 'thể', 'chúng', 'tôi', 'yếu', 'đến', 'có', 'thể', 'cảm', 'thấy', 'như', 'cái', 'chết', 'đang', 'đến', 'rất', 'gần', '.'])\n",
            "(['<cls>', 'I', 'was', 'so', 'shocked', '.', '<eos>'], ['I', 'was', 'so', 'shocked', '.'], ['Tôi', 'đã', 'bị', 'sốc', '.'])\n"
          ]
        }
      ],
      "source": [
        "train_data = list(zip(train_en_prep, train_vi_prep))\n",
        "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "\n",
        "test_data = list(zip(test_en_prep, test_en, test_vi))\n",
        "\n",
        "for i in range(10):\n",
        "  print(train_data[i])\n",
        "for i in range(10):\n",
        "  print(test_data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make Batch"
      ],
      "metadata": {
        "id": "gOo7AC3da6Nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru3H_RSkWwen",
        "outputId": "a6da6b10-aee6-4374-b25e-47dc516f5e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ]
        }
      ],
      "source": [
        "def make_batch(data, batchsize):\n",
        "  bb = []\n",
        "  ben = []\n",
        "  bvi = []\n",
        "  for en,vi in data:\n",
        "    ben.append(en)\n",
        "    bvi.append(vi)\n",
        "    if len(ben) >= batchsize:\n",
        "      bb.append((ben, bvi))\n",
        "      ben = []\n",
        "      bvi = []\n",
        "  if len(ben) > 0:\n",
        "    bb.append((ben, bvi))\n",
        "  return bb\n",
        "\n",
        "train_data = make_batch(train_data, BATCHSIZE)\n",
        "\n",
        "for i in range(2):\n",
        "  print(train_data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding Batch"
      ],
      "metadata": {
        "id": "y61E3hvWWwPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWrvM8ijW1Ok",
        "outputId": "7d0a6703-c751-4beb-ad1f-d296200271a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ]
        }
      ],
      "source": [
        "def padding_batch(b):\n",
        "  maxlen = max([len(x) for x in b])\n",
        "  for tkl in b:\n",
        "    for i in range(maxlen - len(tkl)):\n",
        "      tkl.append('<pad>')\n",
        "\n",
        "def padding(bb):\n",
        "  for ben, bvi in bb:\n",
        "    padding_batch(ben)\n",
        "    padding_batch(bvi)\n",
        "\n",
        "padding(train_data)\n",
        "\n",
        "for i in range(3):\n",
        "  print(train_data[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqpXQ4dyW7Nu",
        "outputId": "9816a562-c85d-401b-ae4b-f05f7fd80740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 444, 49, 96, 678, 16, 49, 884, 429, 823, 96, 22, 1203, 28, 22, 203, 16, 70, 49, 722, 218, 2403, 10, 2271, 178, 545, 9225, 868, 0, 48, 545, 3], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 109, 49, 96, 198, 6154, 48, 3], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 13, 702, 16, 171, 1625, 10, 1318, 21, 365, 1830, 22, 625, 21, 8919, 0, 16, 442, 171, 186, 1834, 210, 56, 22, 1737, 128, 16, 1741, 58, 1317, 16, 1258, 1259, 16, 1512, 76, 22, 2722, 48, 3], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n"
          ]
        }
      ],
      "source": [
        "train_data = [([[vocabidx_en[token] for token in tokenlist] for tokenlist in ben],\n",
        "               [[vocabidx_vi[token] for token in tokenlist] for tokenlist in bvi]) for ben, bvi in train_data]\n",
        "test_data = [([vocabidx_en[token] for token in enprep], en, vi) for enprep, en, vi in test_data]\n",
        "for i in range(3):\n",
        "  print(train_data[i])\n",
        "  print(test_data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Prepare For Model"
      ],
      "metadata": {
        "id": "F0iyJElCXBty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnlmWW_EW9av"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import Transformer\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "5XK0HLZtXIUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0tWNS7IZ-7e"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 900):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embedding"
      ],
      "metadata": {
        "id": "uHCrh1O8XNzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDnH8XEoaLZK"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Transformer"
      ],
      "metadata": {
        "id": "oGvYe1F3Xada"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiaL92yRaWP9"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.01):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Model Transformer"
      ],
      "metadata": {
        "id": "OgW6EkXOXiQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVaMJ3xRap1e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "torch.manual_seed(0)\n",
        "\n",
        "MODELNAME = 'transfomers.model'\n",
        "SRC_VOCAB_SIZE = len(vocablist_en)\n",
        "TGT_VOCAB_SIZE = len(vocablist_vi)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "model = model.to(DEVICE)\n",
        "PAD_IDX= 1\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4RJiRqLbl-A"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == vocabidx_en['<pad>']).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == vocabidx_vi['<pad>']).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Train Model"
      ],
      "metadata": {
        "id": "q7gkB9DNXt-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pd4LHGvarHj"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "  model.train()\n",
        "  all_time= 0\n",
        "  for epoch in range(30):\n",
        "    start= time.time()\n",
        "    loss = 0\n",
        "\n",
        "    for en, vi in train_data:\n",
        "      \n",
        "      en = torch.tensor(en, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      vi = torch.tensor(vi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      tgt_input = vi[:-1, :]\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(en, tgt_input)\n",
        "      y = model(en, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      tgt_out = vi[1:, :]\n",
        "      batchloss = loss_fn(y.reshape(-1, y.shape[-1]), tgt_out.reshape(-1))\n",
        "      batchloss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss + batchloss.item()\n",
        "    end= time.time()\n",
        "    all_time+=(end-start)\n",
        "    print(\"epoch\", epoch, \": loss\", loss, \"Epoch time\",end-start)\n",
        "  torch.save(model.state_dict(), MODELNAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1QWFWZebXyR",
        "outputId": "4fb526e4-36b1-47db-cb7a-a455c60ea39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable) (4.11.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable) (4.1.1)\n",
            "+-------------------------------------------------------------+------------+\n",
            "|                           Modules                           | Parameters |\n",
            "+-------------------------------------------------------------+------------+\n",
            "|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.0.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.0.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.0.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.0.norm2.bias           |    512     |\n",
            "|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.1.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.1.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.1.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.1.norm2.bias           |    512     |\n",
            "|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.2.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.2.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.2.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.2.norm2.bias           |    512     |\n",
            "|               transformer.encoder.norm.weight               |    512     |\n",
            "|                transformer.encoder.norm.bias                |    512     |\n",
            "|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.0.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.0.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.0.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm3.bias           |    512     |\n",
            "|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.1.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.1.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.1.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm3.bias           |    512     |\n",
            "|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.2.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.2.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.2.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm3.bias           |    512     |\n",
            "|               transformer.decoder.norm.weight               |    512     |\n",
            "|                transformer.decoder.norm.bias                |    512     |\n",
            "|                       generator.weight                      |  5460992   |\n",
            "|                        generator.bias                       |   10666    |\n",
            "|                 src_tok_emb.embedding.weight                |  12503040  |\n",
            "|                 tgt_tok_emb.embedding.weight                |  5460992   |\n",
            "+-------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 36060586\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36060586"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "!pip install prettytable\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4If-YDFbDc0",
        "outputId": "72894411-ef4f-4d55-ca70-61a839c8b139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 : loss 31856.313002109528 Epoch time 296.5860278606415\n",
            "epoch 1 : loss 22142.9118277207 Epoch time 294.49730134010315\n",
            "epoch 2 : loss 18984.236869974528 Epoch time 294.8673982620239\n",
            "epoch 3 : loss 17125.048221588368 Epoch time 295.0297706127167\n",
            "epoch 4 : loss 15812.472588128876 Epoch time 293.49594140052795\n",
            "epoch 5 : loss 14811.71393267857 Epoch time 292.86609172821045\n",
            "epoch 6 : loss 14010.947752150067 Epoch time 292.863525390625\n",
            "epoch 7 : loss 13355.04689856153 Epoch time 292.95084047317505\n",
            "epoch 8 : loss 12797.098360885167 Epoch time 293.13957953453064\n",
            "epoch 9 : loss 12295.630556704942 Epoch time 292.7518231868744\n",
            "epoch 10 : loss 11839.979489493548 Epoch time 292.95212841033936\n",
            "epoch 11 : loss 11389.548726429697 Epoch time 292.9348177909851\n",
            "epoch 12 : loss 10987.865016083466 Epoch time 292.7591178417206\n",
            "epoch 13 : loss 10603.159719586372 Epoch time 293.27292823791504\n",
            "epoch 14 : loss 10241.913813168881 Epoch time 293.67407417297363\n",
            "epoch 15 : loss 9913.115199098364 Epoch time 293.19734287261963\n",
            "epoch 16 : loss 9596.550424995134 Epoch time 292.7540020942688\n",
            "epoch 17 : loss 9292.497941798298 Epoch time 292.3838474750519\n",
            "epoch 18 : loss 9006.890349721303 Epoch time 295.5438244342804\n",
            "epoch 19 : loss 8734.947878932348 Epoch time 294.0552306175232\n",
            "epoch 20 : loss 8488.534416490817 Epoch time 295.25914120674133\n",
            "epoch 21 : loss 8247.94343550026 Epoch time 292.4081223011017\n",
            "epoch 22 : loss 8021.574641481566 Epoch time 292.4657371044159\n",
            "epoch 23 : loss 7815.120788442611 Epoch time 291.9923655986786\n",
            "epoch 24 : loss 7612.4631071473705 Epoch time 292.0711262226105\n",
            "epoch 25 : loss 7426.393032021966 Epoch time 292.8850452899933\n",
            "epoch 26 : loss 7248.9492806827475 Epoch time 292.41852736473083\n",
            "epoch 27 : loss 7070.783444340865 Epoch time 292.48143792152405\n",
            "epoch 28 : loss 6917.202063296572 Epoch time 292.68498182296753\n",
            "epoch 29 : loss 6764.838188906924 Epoch time 291.57943630218506\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Test Model"
      ],
      "metadata": {
        "id": "MyQEeaORX1BU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U4Ztkiqrl8s"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, src, max_len, start_symbol):\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = torch.zeros((num_tokens, num_tokens),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    pred = []\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        # next_word = out.squeeze().argmax()\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # print(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == vocabidx_en['<eos>']:\n",
        "            break\n",
        "        pred_y = vocablist_vi[next_word][0]\n",
        "        pred.append(pred_y)\n",
        "    return pred\n",
        "\n",
        "def test():\n",
        "  model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(MODELNAME))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(DEVICE).view(-1, 1)\n",
        "    p = evaluate(model, input, 50, vocabidx_en['<cls>'])\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
        "  print(\"total: \", len(test_data))\n",
        "  print(\"bleu: \", bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM7EIv4gyL1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4329d109-4e20-4558-a8ff-54753ebb4161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total:  1268\n",
            "bleu:  0.15899406373500824\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZl1IS2iCv2l",
        "outputId": "137ff443-ebdd-4b8a-dbf9-26db5d3af28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "          (dropout3): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "          (dropout3): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.01, inplace=False)\n",
              "          (dropout2): Dropout(p=0.01, inplace=False)\n",
              "          (dropout3): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (generator): Linear(in_features=512, out_features=10666, bias=True)\n",
              "  (src_tok_emb): TokenEmbedding(\n",
              "    (embedding): Embedding(24420, 512)\n",
              "  )\n",
              "  (tgt_tok_emb): TokenEmbedding(\n",
              "    (embedding): Embedding(10666, 512)\n",
              "  )\n",
              "  (positional_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.01, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IWSLT15 with Transformer-final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}